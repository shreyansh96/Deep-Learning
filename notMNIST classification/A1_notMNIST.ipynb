{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ./notMNIST_large.tar.gz\n",
      "Found and verified ./notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for ./notMNIST_large. This may take a while. Please wait.\n",
      "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
      "Extracting data for ./notMNIST_small. This may take a while. Please wait.\n",
      "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB0ElEQVR4nG3SO2uUQRjF8f/MOxuX\ngKyQiBAwxE5B8FKJnZcPENAiKMJaiKAfQCsLFRRriQTcgKBs4QUEFRGDYBMRxY3RIpomKEgSlV2M\nYfPO5Vi8ibquTzm/Ys48c6BrjOXUbOvN8aybwFJTVNLEULc5LspHRa+H/7GqgiRFTXXbgVypwKe2\n07KwvV5KRgIx14lZHLy/MdokA/Cx09j0VkFRrSUpaqTT+l/JK+rTrucK0v6O9/UVtriDB/Ja2f23\nVSblldTeBzXlWtr2O5BN5bt7glPi5LMyTSCurJkx9p5ypaAzuBIXlOv75jWz3JBXCjrHOuu4rFzf\nBlYtY1ReijqLhfVDj+Q137dmV+WlpCuVg9VLjz9HKWmuDGAzrslLUruxLElSUlTDANZyvTBJit77\nmKSgJziMzM2jwRWXR4TJAMRXnCGrHw4OUJItzpdNL9DEyd0eDg6CtRn8mPnw7v2XmZGxCD9xjA/7\nEpJj4fXky6l5gQv9CNq46rFYQjJj9UarqAKG1ZW7IxiEOVGzyVolBYzMFmyiF7cBkD1d60lKiSKn\ncpIoY6eJ3t4adXnQn0+/g7FMw9YFaXGQzi6Z8800UQH2vpg9xD/Vtwzs7IFf8DcdSRkFdyUAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABZ0lEQVR4nMWSPWuUQRSFn7u7UUHW\nZAV7wSqoYGVtYaMEbWxsBBGx8w9YqAj+AS3sREhtaxUNWlgIC4looRaSJiBidoWI2fed+1i82Q9k\nsfV0M885c5mZE0DYXj5/7uSxIwH48/vH9bUPdQhAh1NPh6YTpcPVM3QCIlq3fljqMqFZ6trB7Q4B\n7ftWddGcSLOufNgBblgVtZ4eW6s58iZxfLNrQLaqwR4AB5cWsgWwe5q7FjVdu9ADgN7FV6ZavEdf\ntfiyC0REAN11i2qfgerIFTrRJKPDiiPVndYigGyTza0x2UaApRb7fmY0Xoyhs9C/4Fz9Txj/gs6F\nwyZ3dC782uSvH8oYa/pCLxDaXn10YvLZkxmx/J42AIPdZi+/PPv0dnSgwXesUrNMa/Jro+liwuFV\n92pnC1YaZ0qw+KTOmWaaxSxa3IKI9pV3lTMavflsml4DokXv8uP+t33D740HC2efb+28vsQfU/It\nPYCuJtMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB1klEQVR4nF2SP2tUQRTFz537djdm\n/RMxEDBESCVBEQQLC8FYJIIEEYtgooWVX0D0G/gNrJJSbGysXBAEgxISLERJIVgkGlYMIqZYdUne\nvfdY7Mvb9Z1iBi5zfsyZOUAhxWPmpPMtBBUJjrTpZMTeWaRiWO5yddwTIF5fKIcoDz1nTpLOz40K\nOGHiN4O+zwjOQv/DJtxsGvBjC3DcrlxHZJ3mXG3Rgj9HC27PmXj+AjWw8QYUPzFXcFMR5JY6BGuv\nkADcQQyGPLRJD/45lW3TI/ZO90y9Ra5MRgqsbtsLhnh9fiCq4imNxnvADJ3OjZpISR3bZQQ748Dw\nVwadl6EFVnF9xMWx8i1lf1twBBYHnud1GI13Jctklk7nzkgZdWo/IvhrFAAaX+g0LiADEpAwX3MJ\nrHSaQ0NNvEQcRBUIah+mIgGdLgCgcQwAume2UgCKaToryvkAGQDFMo2ku5mZmTtJ4/skAsHxHUbV\nyfCL0Ezt2pgrkH8vo53MAM8W1wUJLRpzLmtdVVW1pk+Y09k+DGCyy2Bwul+KOTrpvCHAfRqdn+pS\nKMnRNp0Wz4D0jsacjw5KBSiWmDO4O4FLeTAiP9f/QcVMj/sQSzQa16RfVcHwJp0eH/8B+FMx3quI\nG+YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(filename=\"notMNIST_small/A/Q0NXaWxkV29yZHMtQm9sZEl0YWxpYy50dGY=.png\"))\n",
    "display(Image(filename=\"notMNIST_large/A/a2F6b28udHRm.png\"))\n",
    "display(Image(filename=\"notMNIST_large/A/UXVlYmVjLUV4dHJhQm9sZC5vdGY=.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  9.21956299e-41,   9.21956299e-41,   9.18368975e-41],\n",
       "        [  9.18368975e-41,   2.36935608e-38,   2.36942783e-38],\n",
       "        [  2.36942783e-38,   2.36942783e-38,   2.36942783e-38]],\n",
       "\n",
       "       [[  2.36942783e-38,   2.36942783e-38,   2.36942783e-38],\n",
       "        [  2.36942783e-38,   2.36942783e-38,   2.36942783e-38],\n",
       "        [  2.36942783e-38,   2.36942783e-38,   2.36942783e-38]]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of ndarray shape\n",
    "np.ndarray(shape=(2,3,3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling ./notMNIST_large/A.pickle.\n",
      "./notMNIST_large/A\n",
      "Could not read: ./notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : cannot identify image file './notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png' - it's ok, skipping.\n",
      "Could not read: ./notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file './notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png' - it's ok, skipping.\n",
      "Could not read: ./notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file './notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52909, 28, 28)\n",
      "Mean: -0.12825\n",
      "Standard deviation: 0.443121\n",
      "Pickling ./notMNIST_large/B.pickle.\n",
      "./notMNIST_large/B\n",
      "Could not read: ./notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file './notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.00756304\n",
      "Standard deviation: 0.454492\n",
      "Pickling ./notMNIST_large/C.pickle.\n",
      "./notMNIST_large/C\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.142258\n",
      "Standard deviation: 0.439806\n",
      "Pickling ./notMNIST_large/D.pickle.\n",
      "./notMNIST_large/D\n",
      "Could not read: ./notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file './notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.0573678\n",
      "Standard deviation: 0.455648\n",
      "Pickling ./notMNIST_large/E.pickle.\n",
      "./notMNIST_large/E\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.069899\n",
      "Standard deviation: 0.452942\n",
      "Pickling ./notMNIST_large/F.pickle.\n",
      "./notMNIST_large/F\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.125583\n",
      "Standard deviation: 0.44709\n",
      "Pickling ./notMNIST_large/G.pickle.\n",
      "./notMNIST_large/G\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0945814\n",
      "Standard deviation: 0.44624\n",
      "Pickling ./notMNIST_large/H.pickle.\n",
      "./notMNIST_large/H\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.068522\n",
      "Standard deviation: 0.454231\n",
      "Pickling ./notMNIST_large/I.pickle.\n",
      "./notMNIST_large/I\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: 0.0307862\n",
      "Standard deviation: 0.468899\n",
      "Pickling ./notMNIST_large/J.pickle.\n",
      "./notMNIST_large/J\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.153359\n",
      "Standard deviation: 0.443657\n",
      "Pickling ./notMNIST_small/A.pickle.\n",
      "./notMNIST_small/A\n",
      "Could not read: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file './notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.132626\n",
      "Standard deviation: 0.445128\n",
      "Pickling ./notMNIST_small/B.pickle.\n",
      "./notMNIST_small/B\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: 0.00535608\n",
      "Standard deviation: 0.457115\n",
      "Pickling ./notMNIST_small/C.pickle.\n",
      "./notMNIST_small/C\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.141521\n",
      "Standard deviation: 0.44269\n",
      "Pickling ./notMNIST_small/D.pickle.\n",
      "./notMNIST_small/D\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0492167\n",
      "Standard deviation: 0.459759\n",
      "Pickling ./notMNIST_small/E.pickle.\n",
      "./notMNIST_small/E\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0599147\n",
      "Standard deviation: 0.45735\n",
      "Pickling ./notMNIST_small/F.pickle.\n",
      "./notMNIST_small/F\n",
      "Could not read: ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file './notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.118185\n",
      "Standard deviation: 0.452279\n",
      "Pickling ./notMNIST_small/G.pickle.\n",
      "./notMNIST_small/G\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0925503\n",
      "Standard deviation: 0.449006\n",
      "Pickling ./notMNIST_small/H.pickle.\n",
      "./notMNIST_small/H\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0586893\n",
      "Standard deviation: 0.458759\n",
      "Pickling ./notMNIST_small/I.pickle.\n",
      "./notMNIST_small/I\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: 0.0526451\n",
      "Standard deviation: 0.471893\n",
      "Pickling ./notMNIST_small/J.pickle.\n",
      "./notMNIST_small/J\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.151689\n",
      "Standard deviation: 0.448014\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngUL9P4qaHKPE4Md3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26a9fa4978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## checking some random images\n",
    "pickle_file=train_datasets[0]\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    letter_set=pickle.load(f) #unpickling the files\n",
    "    sample_idx=np.random.randint(len(letter_set))\n",
    "    sample_image=letter_set[sample_idx,:,:]\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image, cmap='winter')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Train Dataset ...\n",
      "A  -->  52909\n",
      "B  -->  52911\n",
      "C  -->  52912\n",
      "D  -->  52911\n",
      "E  -->  52912\n",
      "F  -->  52912\n",
      "G  -->  52912\n",
      "H  -->  52912\n",
      "I  -->  52912\n",
      "J  -->  52911\n",
      "Check for Test Dataset ...\n",
      "A  -->  1872\n",
      "B  -->  1873\n",
      "C  -->  1873\n",
      "D  -->  1873\n",
      "E  -->  1873\n",
      "F  -->  1872\n",
      "G  -->  1872\n",
      "H  -->  1872\n",
      "I  -->  1872\n",
      "J  -->  1872\n"
     ]
    }
   ],
   "source": [
    "#checking data is balanced\n",
    "# we can also check the data is balanced across all classes using the output when pickling was done, still to be sure!!!\n",
    "print(\"Check for Train Dataset ...\")\n",
    "for pickle_file in train_datasets:\n",
    "    with open(pickle_file,'rb') as f:\n",
    "        letter_set=pickle.load(f)\n",
    "        print(pickle_file[-8:-7],' --> ', len(letter_set))\n",
    "print(\"Check for Test Dataset ...\")\n",
    "for pickle_file in test_datasets:\n",
    "    with open(pickle_file,'rb') as f:\n",
    "        letter_set=pickle.load(f)\n",
    "        print(pickle_file[-8:-7],' --> ', len(letter_set))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject=open(train_datasets[1],'rb')\n",
    "a=pickle.load(fileObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, './notMNIST_small/A.pickle')\n",
      "(1, './notMNIST_small/B.pickle')\n",
      "(2, './notMNIST_small/C.pickle')\n",
      "(3, './notMNIST_small/D.pickle')\n",
      "(4, './notMNIST_small/E.pickle')\n",
      "(5, './notMNIST_small/F.pickle')\n",
      "(6, './notMNIST_small/G.pickle')\n",
      "(7, './notMNIST_small/H.pickle')\n",
      "(8, './notMNIST_small/I.pickle')\n",
      "(9, './notMNIST_small/J.pickle')\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(test_datasets):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800503\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1Ddata=np.zeros([test_dataset.shape[0],784])\n",
    "for i in range(test1Ddata.shape[0]):\n",
    "    test1Ddata[i,:]=test_dataset[i,:,].ravel()\n",
    "\n",
    "train1Ddata=np.zeros([train_dataset.shape[0],784])\n",
    "for i in range(train1Ddata.shape[0]):\n",
    "    train1Ddata[i,:]=train_dataset[i,:,].ravel()\n",
    "\n",
    "valid1Ddata=np.zeros([valid_dataset.shape[0],784])\n",
    "for i in range(valid1Ddata.shape[0]):\n",
    "    valid1Ddata[i,:]=valid_dataset[i,:,].ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68,)\n",
      "(1138,)\n",
      "(969,)\n",
      "20.629316568374634\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "nrows, ncols = test1Ddata.shape\n",
    "dtype={'names':['f{}'.format(i) for i in range(ncols)],\n",
    "       'formats':ncols * [test1Ddata.dtype]}\n",
    "nrows,ncols=train1Ddata.shape\n",
    "traindtype={'names':['f{}'.format(i) for i in range(ncols)],\n",
    "       'formats':ncols * [train1Ddata.dtype]}\n",
    "#print(dtype)\n",
    "print(np.intersect1d(valid1Ddata.view(dtype),test1Ddata.view(dtype)).shape)\n",
    "print(np.intersect1d(train1Ddata.view(traindtype),test1Ddata.view(dtype)).shape)\n",
    "print(np.intersect1d(valid1Ddata.view(dtype),train1Ddata.view(traindtype)).shape)\n",
    "\n",
    "\n",
    "total=time.time()-start\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlaps between training and test sets: 1138. Execution time: 0.4166349999999852.\n",
      "Number of overlaps between training and validation sets: 969. Execution time: 0.38521800000000894.\n",
      "Number of overlaps between validation and test sets: 68. Execution time: 0.039141999999998234.\n"
     ]
    }
   ],
   "source": [
    "def check_overlaps(images1, images2):\n",
    "    images1.flags.writeable=False\n",
    "    images2.flags.writeable=False\n",
    "    start = time.clock()\n",
    "    hash1 = set([hash(image1.tobytes()) for image1 in images1])\n",
    "    hash2 = set([hash(image2.tobytes()) for image2 in images2])\n",
    "    all_overlaps = set.intersection(hash1, hash2)\n",
    "    return all_overlaps, time.clock()-start\n",
    "\n",
    "r, execTime = check_overlaps(train_dataset, test_dataset)    \n",
    "print('Number of overlaps between training and test sets: {}. Execution time: {}.'.format(len(r), execTime))\n",
    "\n",
    "r, execTime = check_overlaps(train_dataset, valid_dataset)   \n",
    "print('Number of overlaps between training and validation sets: {}. Execution time: {}.'.format(len(r), execTime))\n",
    "\n",
    "r, execTime = check_overlaps(valid_dataset, test_dataset) \n",
    "print('Number of overlaps between validation and test sets: {}. Execution time: {}.'.format(len(r), execTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(multi_class='multinomial', solver='lbfgs',random_state=42, verbose=1, max_iter=1000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 11.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
       "          n_jobs=-1, penalty='l2', random_state=42, solver='lbfgs',\n",
       "          tol=0.0001, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train1Ddata,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89749999999999996"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test1Ddata,test_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
